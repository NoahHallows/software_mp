import sys
from PySide6.QtWidgets import QApplication, QDialog, QDialogButtonBox, QLineEdit, QVBoxLayout, QFileDialog, QProgressBar, QRadioButton, QPushButton, QLabel, QGridLayout, QListWidget, QMessageBox
from PySide6.QtCore import Slot
from PySide6.QtGui import QPixmap
import re
import cv2 as cv
import os
from time import sleep
from multiprocessing import cpu_count, Queue, Pool, Value, Process, Event, freeze_support
import pathlib
import face_recognition
import concurrent.futures
progress <- Value('i', 0)
images_to_search <- []
event <- Event()
data_queue <- Queue()

CLASS Window(QDialog):
    # Logic for getting radio button value
    @Slot()
    FUNCTION radio(self):
        button <-  sender()
         action <- button.option
    # For end screen
    ENDFUNCTION

    FUNCTION end_screen(self, results):
         close()
        end_grid <- QGridLayout()
        FOR image_name IN results:
            #image_name <- image_name + ".temp"
            image <- cv.imread(image_name)
            # Get the dimensions of the image (height, width, number_of_channels)
            height, width, channels <- image.shape
            scale_factor <- 200/height
            new_width <- int(round(scale_factor*width, 0))
            resized_image <- cv.resize(image, (new_width, 200), interpolation=cv.INTER_AREA)
            imgbytes <- cv.imencode(".png", resized_image)[1].tobytes()
            image_display <-  QLabel(self)
            pixmap <- QPixmap(resized_image)
            image_display.setPixmap(pixmap)
            end_grid.addWidget(image_display, 0,0)
        ENDFOR
    # Logic for selecting target image
    ENDFUNCTION

    @Slot()
    FUNCTION select_target_face(self):
        target_face_location_untrimmed <- QFileDialog.getOpenFileName(self, ("Open image"), "", ("Image (*.png *.jpg *.bmp)"))
        IF target_face_location_untrimmed:
             target_image_text_box.clear()
            # Use regular expression to find the file path
            # THIS STRING PROCESSING IS A STARDARD ALGORITHM
             target_face_location <- Extract from 2nd character until character == "'" from target_face_location_untrimmed into target_face_location
             target_image_text_box.setText( target_face_location)
        ENDIF
    # Logic for progress bar
    ENDFUNCTION

    FUNCTION progress_bar_update(self, images_to_search):
        percentage <- 0
        WHILE percentage <= 100:
            percentage <- progress.value * (100/len(images_to_search))
             progress_bar.setValue(percentage)
            sleep(0.5)
        ENDWHILE
    # For standard buttons
    ENDFUNCTION

    @Slot()
    FUNCTION accept(self):
        # Input checking
        IF  images_to_search_location != '' AND  action != 0 AND  target_face_location != '':
            # Setup worker to update progress bar
            progress_worker <- concurrent.futures.ThreadPoolExecutor(max_workers=1)
            progress_worker.submit( progress_bar_update,  images_to_search)
            #images_to_search <- get_files( images_to_search_location)
            # Use of queues to transfer data from frontend to backend
            data_queue.put([ action,  target_face_location,  images_to_search,  overlay_image_location])
            # Tell the backend to run
            event.set()
        ELSE:
            msgBox <- QMessageBox()
                                             ENDFOR
            msgBox.exec()
        ENDIF
    ENDFUNCTION

    @Slot()
    FUNCTION cancel(self):
         close()
    ENDFUNCTION

    FUNCTION __init__(self):
        # Set overlay_image_location to none in case it's not applicable
         overlay_image_location <- None
        super().__init__(parent=None)
         setWindowTitle("Face removal tool v0.8")
        dialogLayout <- QVBoxLayout()
        #formLayout <- QFormLayout()
         ENDFOR
        gridLayout <- QGridLayout()
        # Selecting target face image
        target_image_button <- QPushButton("Browse")
         target_image_text_box <- QLineEdit()
         target_image_text_box.setReadOnly(True)
        gridLayout.addWidget(QLabel("Select the image containing the face to search for:"), 0, 0)
                                                                                    ENDFOR
        gridLayout.addWidget( target_image_text_box, 0, 1)
        gridLayout.addWidget(target_image_button, 0, 2)
        target_image_button.clicked.connect( select_target_face)
        #Selecting image directory
        select_image_directory_button <- QPushButton("Browse")
         select_image_directory_text_box <- QLineEdit()
         select_image_directory_text_box.setReadOnly(True)
        gridLayout.addWidget(QLabel("Select the directory contining images to search:"), 1, 0)
        gridLayout.addWidget( select_image_directory_text_box, 1, 1)
        gridLayout.addWidget(select_image_directory_button, 1, 2)
        select_image_directory_button.clicked.connect( select_image_directory)
        # List images
         image_list_box <- QListWidget()
        gridLayout.addWidget( image_list_box, 2, 0, 1, 2)
        # Select action option
        blur_button <- QRadioButton("Blur matching faces", self)
        blur_button.option <- 1
        replace_button <- QRadioButton("Replace matching faces", self)
        replace_button.option <- 2
        delete_button <- QRadioButton("Delete images containg matching faces", self)
        delete_button.option <- 3
        # Add radio buttons to layout
        gridLayout.addWidget(QLabel("What should the program do to matching faces:"), 3, 0, 1, 2)
        gridLayout.addWidget(blur_button, 4, 0)
        gridLayout.addWidget(replace_button, 5, 0)
        gridLayout.addWidget(delete_button, 6, 0)
        # Connect radio buttons to fuction
        blur_button.clicked.connect( radio)
        replace_button.clicked.connect( radio)
        delete_button.clicked.connect( radio)
        # Select overlay image IF applicable
        select_overlay_button <- QPushButton("Browse")
         select_overlay_text_box <- QLineEdit()
         select_overlay_text_box.setReadOnly(True)
        gridLayout.addWidget(QLabel("Select the overlay image:"), 7, 0)
        gridLayout.addWidget( select_overlay_text_box, 7, 1)
        gridLayout.addWidget(select_overlay_button, 7, 2)
        select_overlay_button.clicked.connect( select_overlay_image)
        # Add formLayout to dialogLayout
        dialogLayout.addLayout(gridLayout)
        # Progress bar
         progress_bar <- QProgressBar()
        dialogLayout.addWidget( progress_bar)
        # Add standard buttons
         standard_buttons <- QDialogButtonBox(QDialogButtonBox.Ok
                             = QDialogButtonBox.Cancel)
        dialogLayout.addWidget( standard_buttons)
        # Connect the accepted AND rejected signals to respective methods
         standard_buttons.accepted.connect( accept)
         standard_buttons.rejected.connect( reject)
        # Set dialogLayout to be the layout of the window
         setLayout(dialogLayout)
    # Logic for selecting overlay image
    ENDFUNCTION

    @Slot()
    FUNCTION select_overlay_image(self):
        overlay_image_location_untrimmed <- QFileDialog.getOpenFileName(self, ("Open image"), "", ("Image (*.png *.jpg *.bmp)"))
        IF overlay_image_location_untrimmed:
             select_overlay_text_box.clear()
            # Use regular expression to find the file path
            Extract from 2nd character until character == "'" from overlay_image_location_untrimmed into overlay_image_location
             select_overlay_text_box.setText(overlay_image_location)
        ENDIF
    # Logic for selecting image directory
    ENDFUNCTION

            ENDFOR
    @Slot()
    FUNCTION select_image_directory(self):
         images_to_search_location <- QFileDialog.getExistingDirectory(self, ("Open folder"))
        IF  images_to_search_location:
             select_image_directory_text_box.clear()
             select_image_directory_text_box.setText(str( images_to_search_location))
            # Display images in directory
             images_to_search <- get_files( images_to_search_location)
            FOR image IN images_to_search:
                 image_list_box.addItem(image)
            ENDFOR
        ENDIF
    ENDFUNCTION

ENDCLASS

            ENDFOR
CLASS backend:
    FUNCTION __init__(self):
        # Create backend windows for popups
                                 ENDFOR
        app <- QApplication([])
        window <- Window()
        # Wait for trigger
               ENDFOR
        event.wait()
        # Get input data
        input_data <- data_queue.get()
         action <- input_data[0]
         target_face_location <- input_data[1]
         images_to_search <- input_data[2]
         overlay_image_location <- input_data[3]
        # Call function to spawn processing threads
         start( images_to_search,  action)
    ENDFUNCTION

    FUNCTION start(self, images_to_search, action):
        progress.value <- 0
        # Process the image contaning the face to search fOR
                                                         ENDFOR
        TRY:
            face_to_search_for <- face_recognition.load_image_file( target_face_location)
                           ENDFOR
             face_to_search_for_encoding <- face_recognition.face_encodings(face_to_search_for)[0]
                                ENDFOR
        EXCEPT Exception as e:
            OUTPUT f"Error {e}"
            msgBox <- QMessageBox()
            msgBox.setText(f"No face found in image containing target face\n{e}")
            msgBox.exec()
        ENDTRY
        IF action = 2:
            #if selected access the overlay image
             overlay <- cv.imread( overlay_image_location)
        ENDIF
        WITH Pool(processes=cpu_count()) as pool:
            # Map the image processing function over the images
            results <- pool.map( face_recog, images_to_search)
        results <- [item FOR item in results IF item is not None]
        msgBox <- QMessageBox()
        msgBox.setText("The images have been searched")
        msgBox.exec()
    ENDFUNCTION

    FUNCTION face_recog(self, image_name):
        TRY:
            # Load AND run face recognition on the image to search
            image <- face_recognition.load_image_file(image_name)
            image_encodings <- face_recognition.face_encodings(image)
            IF image_encodings:
                image_encoding <- image_encodings[0]
                # Compare faces
                results <- face_recognition.compare_faces([ face_to_search_for_encoding], image_encoding)
                # Convert image to BGR for OpenCV
                image_bgr <- cv.cvtColor(image, cv.COLOR_RGB2BGR)
                IF results[0]:
                    # Get location of faces in image
                    face_locations <- face_recognition.face_locations(image)
                    FOR face_location in face_locations:
                        # See IF the face is a match for the known face
                        face_encoding <- face_recognition.face_encodings(image, [face_location])[0]
                        match <- face_recognition.compare_faces([ face_to_search_for_encoding], face_encoding)
                        # If it's a match, blur the face
                        IF match[0]:
                            IF  action = 1:
                                # Call editing image CLASS blur function
                                new_image <- editing_image.blur(image_bgr, face_location, 1)
                            ELSEIF  action = 2:
                                new_image <- editing_image.replace(image_bgr,  overlay, face_location, 1)
                            ELSEIF  action = 3:
                                os.remove(image_name)
                            ENDIF
                            cv.imwrite(image_name, new_image)
                            progress.value += 1
                        ENDIF
                        RETURN image_name
                    ENDFOR
                ELSE:
                    # Put progress update to the queue
                    progress.value += 1
                    RETURN f"Image {image_name} doesn't match"
                ENDIF
            ELSE:
                # Put progress update to the queue
                progress.value += 1
                RETURN f"No faces found in image {image_name}"
            ENDIF
        EXCEPT Exception as e:
            progress.value += 1
            RETURN f"An error occurred with image {image_name}: {e}"
        ENDTRY
    ENDFUNCTION

ENDCLASS

FUNCTION get_files(images_to_search_location):
    FOR file in pathlib.Path(images_to_search_location).rglob("*.jpg"):
        images_to_search.append(str(file))
    ENDFOR
    FOR file in pathlib.Path(images_to_search_location).rglob("*.JPG"):
        images_to_search.append(str(file))
    ENDFOR
    FOR file in pathlib.Path(images_to_search_location).rglob("*.png"):
        images_to_search.append(str(file))
    ENDFOR
    RETURN list(images_to_search)
ENDFUNCTION

CLASS editing_image():
    #EDITING IMAGES
    FUNCTION blur(image_bgr, target_face_location, scale_factor):
        # Unpack the location
        top, right, bottom, left <- target_face_location
        # Scale face_location coordinates up to the original image size
        top <- int(top / scale_factor)
        right <- int(right / scale_factor)
        bottom <- int(bottom / scale_factor)
        left <- int(left / scale_factor)
        # Calculate the width AND height of the bounding box
        width <- right - left
        height <- bottom - top
        # Increase the region slightly to make sure the entire face is covered
        top <- max(top - 10, 0)
        right <- min(right + 10, image_bgr.shape[1])
        bottom <- min(bottom + 10, image_bgr.shape[0])
        left <- max(left - 10, 0)
        # Select the region of interest (ROI) where the face is located
        face_roi <- image_bgr[top:bottom, left:right]
        # Apply a Gaussian blur to the face region
        blurred_face <- cv.GaussianBlur(face_roi, (999, 999), 0)
        # Replace the original image region with the blurred face
        image_bgr[top:bottom, left:right] <- blurred_face
        RETURN image_bgr
    ENDFUNCTION

    FUNCTION replace(background, overlay, target_face_location, scale_factor):
        # Unpack the location
        top, right, bottom, left <- target_face_location
        # Scale face_location coordinates up to the original image size
        top <- int(top / scale_factor)
        right <- int(right / scale_factor)
        bottom <- int(bottom / scale_factor)
        left <- int(left / scale_factor)
        # Calculate the width AND height of the bounding box
        width <- right - left
        height <- botton - top
        # Expand the bounding box by the constant value
        top <- max(0, top - 10)
        bottom <- min(background.shape[0], bottom + 10)
        left <- max(0, left - 10)
        right <- min(background.shape[1], right + 10)
        width <- width + 20
        height <- height + 20
        # Ensure the width AND height are positive before resizing
        IF width > 0 AND height > 0:
            overlay_resized <- cv.resize(overlay, (width, height))
        ELSE:
            # Handle the invalid size case, e.g., by skipping the resizing OR setting a default size
            OUTPUT f"Invalid size for resize operation: width={width}, height={height}"
            RETURN background
	ENDIF
        # Check IF overlay image has an alpha channel (transparency)
        IF overlay_resized.shape[2] = 4:
            # Split overlay into color AND alpha channels
            overlay_color <- overlay_resized[:, :, :3]
            alpha_mask <- overlay_resized[:, :, 3] / 255.0
            # Get the ROI from the background AND blend using the alpha mask
            roi <- background[top:bottom, left:right]
            roi <- cv.addWeighted(overlay_color, alpha_mask, roi, 1 - alpha_mask, 0, roi)
            # Put the blended ROI back into the background
            background[top:bottom, left:right] <- roi
        ELSE:
            # If no alpha channel, just replace the ROI with the resized overlay
            background[top:bottom, left:right] <- overlay_resized
        ENDIF
        RETURN background
    ENDFUNCTION

ENDCLASS

FUNCTION ui_start():
    app <- QApplication([])
    window <- Window()
    window.show()
    sys.exit(app.exec())
ENDFUNCTION

FUNCTION main():
    # For pyinstaller
    freeze_support()
    # creating processes
    frontend_thread <- Process(target=ui_start)
    backend_thread <- Process(target=backend)
    frontend_thread.start()
    backend_thread.start()
    frontend_thread.join()
    backend_thread.join()
ENDFUNCTION

IF __name__ = "__main__":
    main()
ENDIF
